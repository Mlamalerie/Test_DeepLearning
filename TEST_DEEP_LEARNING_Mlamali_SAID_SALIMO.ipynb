{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST_DEEP_LEARNING_Mlamali_SAID_SALIMO.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLsjlE0irZZPMa2yIdVpvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mlamalerie/Test_DeepLearning/blob/main/TEST_DEEP_LEARNING_Mlamali_SAID_SALIMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST DEEP LEARNING\n"
      ],
      "metadata": {
        "id": "WkrTEULkNvpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 :\n",
        "\n",
        "**Décrivez un pipeline de détection de langage abusif sur un réseau social. Vous pouvez utiliser des schémas.**"
      ],
      "metadata": {
        "id": "A-Ya4FuHN0cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La détection du langage abusif est une tâche complexe qui doit s'adapter à différentes formes de discours et de langage. Pour ce faire, nous avons besoin d'une base de données de ce réseau social, ou alors nous pouvons faire du webscraping sur celui-ci en se basant sur des mot clé ou des hashtag pour trouver les messages les plus pertinents pour notre problème.\n",
        "\n",
        "On a ici un problème de classification binaire de texte issue de domaine du NLP (Natural Language Processing)\n",
        "\n",
        "Tout d'abord, nous avons une phase d'extraction de données. Celle-ci peut être fait par un ETL (Extract-transform-load) pour les bases de données ou du scraping de page web du réseau social. Après vient une phase de prétraitement, ou on va apporter des modifications aux données afin de tirer que les informations les plus importantes. D'ailleurs, généralement, ces traitements sont très gourmands en calcul, c'est pour cela que des jobs spark ou des fonctions Lambda (AWS) sont utilisés sur des clusters et donc tirer profit de la parallélisation. Cette partie est très importante. Elle nous permettra d'avoir un bon corpus de données et ainsi avoir une bonne phase d'extraction de données, ou on aura une représentation vectorielle de nos données, on peut tout de meme utiliser un modéle pré-entrainé sur un large corpus de données, ensuite ces données seront utilisée pour la création d'un modèle prédictif ayant pour but de détecter le contenu abusif en classification binaire 0 ou 1."
      ],
      "metadata": {
        "id": "W5n4-djNN_Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 :"
      ],
      "metadata": {
        "id": "0-sdMZFO_N4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expliquez le processus permettant d’utiliser un algorithme hybride CNN-LSTM pour détecter un contenu agressif dans une publication.\n",
        "\n",
        "CNN-LSTM for detect aggressive content in a post\n",
        "\n",
        "l'objectif de cette tache est de détecter les termes agressif contenu dans une publication donnée, ce qui rend cette tache difficile, c'est le vocabulaire varié, et des contextes différent et lié.\n",
        "\n",
        "Pour cela on peut utiliser une architecture hybride Cnn-Lstm:\n",
        "\n",
        "D'abords le modèle aura en entrer les séquence de texte ainsi que leur label (si contenu agressif ou pas) ensuite on aura une couche de Word-embbeding qui aura comme input une matrice de séquence de texte qui vont être représenté sous forme vectorielle(wod2vec), juste après viendra un CNN qui va être utilisé comme features extractor donc il aura pour mission d'extraire les informations locale en utilisant des filtres 1D et du Pooling afin de réduire la complexité et la taille des données, ensuite les sorties du CNN (feature vector) seront envoyés comme entré au LSTM, ce dernier va utiliser l'historique et ces features afin de trouver le contexte du texte , puis enfin s'ajoute une couche de fully connected (dense) et un softmax ou sigmoid, qui aura pour but de classifier la séquence en entrer."
      ],
      "metadata": {
        "id": "0d6n1OBP_3Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_j6oME14N4jA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}