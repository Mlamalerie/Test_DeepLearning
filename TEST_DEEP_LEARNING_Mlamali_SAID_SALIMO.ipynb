{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST_DEEP_LEARNING_Mlamali_SAID_SALIMO.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObLGbH2UUKWr213OqMS3Ip",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mlamalerie/Test_DeepLearning/blob/main/TEST_DEEP_LEARNING_Mlamali_SAID_SALIMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST DEEP LEARNING\n"
      ],
      "metadata": {
        "id": "WkrTEULkNvpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 :\n",
        "\n",
        "**Décrivez un pipeline de détection de langage abusif sur un réseau social. Vous pouvez utiliser des schémas.**"
      ],
      "metadata": {
        "id": "A-Ya4FuHN0cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La détection du langage abusif est une tâche complexe qui doit s'adapter à différentes formes de discours et de langage. Pour ce faire, nous avons besoin d'une base de données de ce réseau social, ou alors nous pouvons faire du webscraping sur celui-ci en se basant sur des mot clé ou des hashtag pour trouver les messages les plus pertinents pour notre problème.\n",
        "\n",
        "On a ici un problème de classification binaire de texte issue de domaine du NLP (Natural Language Processing)\n",
        "\n",
        "Tout d'abord, nous avons une phase d'extraction de données. Celle-ci peut être fait par un ETL pour les bases de données ou du scraping de page web du réseau social, après vient une phase de prétraitement, ou on va apporter des modifications aux données afin de tirer que les informations les plus importantes, généralement, c'est traitements sont très gourmands en calcule, c'est pour cela que des jobs spark ou des fonctions Lambda (AWS) sont utilisés sur des clusters et donc tirer profit de la parallélisation, cette partie est très importante elle nous permettra d'avoir un bon corpus de données et ainsi avoir une bonne phase d'extraction de données, ou on aura une représentation vectorielle de nos données, on peut tout de meme utiliser un modéle pré-entrainé sur un large corpus de données, ensuite ces données seront utilisée pour la création d'un modèle prédictif ayant pour but de détecter le contenu abusif en classification binaire 0 ou 1."
      ],
      "metadata": {
        "id": "W5n4-djNN_Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_j6oME14N4jA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}